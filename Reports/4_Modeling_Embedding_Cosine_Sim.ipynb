{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOp5r9EJwk4p"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xRUmcyvCJsem",
    "outputId": "ace3ea5a-36fe-4eb3-a960-f7abfc267afd"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive/My Drive/Colab Notebooks/Multi_Label_Text_Classification\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "sys.path.append('/content/gdrive/My Drive/Colab Notebooks/Multi_Label_Text_Classification')\n",
    "base_dir = 'gdrive/My Drive/Colab Notebooks/Multi_Label_Text_Classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "izPpn887fH5Q"
   },
   "outputs": [],
   "source": [
    "!pip3 install --quiet \"tensorflow>=1.7\"\n",
    "!pip3 install --quiet tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T19:02:17.597554Z",
     "start_time": "2019-03-12T19:01:56.079763Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "pr_bmQWWaG9C",
    "outputId": "2f5847c9-30c8-4aaa-df3b-a8457782372c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob\n",
    "import functools \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import re\n",
    "import os.path\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "#from sklearn.cross_validation import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scipy\n",
    "\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\", font_scale=0.8)\n",
    "\n",
    "from helper_functions import *\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5fm4UOvgcT9"
   },
   "source": [
    "# Few relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OX_BCx0ia-W"
   },
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation):\n",
    "    unique_labels = functools.reduce(lambda l, x: l if x in l else l+[x], labels, [])\n",
    "    corr = np.inner(features, features)\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    sns.set(font_scale=1.2)\n",
    "    g = sns.heatmap(\n",
    "        corr,\n",
    "        xticklabels=unique_labels,\n",
    "        yticklabels=unique_labels,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cmap=\"YlOrRd\")\n",
    "    g.set_xticklabels(unique_labels, rotation=rotation)\n",
    "    ticks = np.linspace(len(features)/len(unique_labels)/2, len(features)-len(features)/len(unique_labels)/2, len(unique_labels), dtype=np.int)\n",
    "    g.set_xticks(ticks)\n",
    "    g.set_yticks(ticks)\n",
    "    g.set_title(\"Semantic Textual Similarity for: {}\".format(unique_labels))\n",
    "\n",
    "\n",
    "def run_and_plot(messages_, labels):\n",
    "    similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "    similarity_message_encodings = embed(similarity_input_placeholder)\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())  \n",
    "        message_embeddings_ = session.run(similarity_message_encodings, feed_dict={similarity_input_placeholder: messages_})\n",
    "        plot_similarity(labels, message_embeddings_, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1MdQma6ujkSL"
   },
   "outputs": [],
   "source": [
    "def get_2d_representation(words_list, labels):\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        description_embeddings  =  session.run(embed(words_list)) #Generates the sentence embeddings\n",
    "        \n",
    "        model = PCA(n_components = 2) # Initializing the PCA object. We use n_components = 2 to help us plot our findings in the 2D space.\n",
    "        # Apply the fit_transform method of model to grains: pca_features\n",
    "        pca_features = model.fit_transform(description_embeddings) # Transforms embeddings to vectors of size 2\n",
    "        # Assign 0th column of pca_features: xs\n",
    "        xs = pca_features[:,0] #The first component of PCA\n",
    "        # Assign 1st column of pca_features: ys\n",
    "        ys = pca_features[:,1] #Second component of PCA\n",
    "        \n",
    "        #Next, we'll plot these results\n",
    "        tmp = {}\n",
    "        label_idx = [tmp.setdefault(name, len(tmp)) for name in labels]\n",
    "        unique_labels = functools.reduce(lambda l, x: l if x in l else l+[x], labels, [])\n",
    "        df = pd.DataFrame({'x':xs, 'y': ys, 'label':label_idx})\n",
    "        colors = ['r', 'b', 'g', 'm', 'k']\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10,10)) \n",
    "        for idx in range(len(set(labels))):\n",
    "          ax.scatter(df[df['label']==idx].x, df[df['label']==idx].y, c=colors[idx], label=unique_labels[idx])\n",
    "        \n",
    "        ax.set(title='PCA Representation for Genres: {}'.format(unique_labels))\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3C7whsn4GIa_"
   },
   "outputs": [],
   "source": [
    "def movie_genre_heatmap(features, labels):\n",
    "    unique_labels = functools.reduce(lambda l, x: l if x in l else l+[x], labels, [])\n",
    "    corr = np.inner(features, features)\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    sns.set(font_scale=1.2)\n",
    "    g = sns.heatmap(\n",
    "        corr,\n",
    "        xticklabels=unique_labels,\n",
    "        yticklabels=unique_labels,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cmap=\"YlOrRd\")\n",
    "    g.set_xticklabels(unique_labels, rotation=90)\n",
    "    ticks = np.linspace(len(features)/len(unique_labels)/2, len(features)-len(features)/len(unique_labels)/2, len(unique_labels), dtype=np.int)\n",
    "    g.set_xticks(ticks)\n",
    "    g.set_yticks(ticks)\n",
    "    g.set_title(\"Semantic Textual Similarity for: {}\".format(unique_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SUlZgoRyHHkM"
   },
   "outputs": [],
   "source": [
    "def movie_genre_scatter_plot(movie_plot_vectors, labels):\n",
    "    model = PCA(n_components = 2) # Initializing the PCA object. We use n_components = 2 to help us plot our findings in the 2D space.\n",
    "    # Apply the fit_transform method of model to grains: pca_features\n",
    "    pca_features = model.fit_transform(movie_plot_vectors) # Transforms embeddings to vectors of size 2\n",
    "    # Assign 0th column of pca_features: xs\n",
    "    xs = pca_features[:,0] #The first component of PCA\n",
    "    # Assign 1st column of pca_features: ys\n",
    "    ys = pca_features[:,1] #Second component of PCA\n",
    "\n",
    "    #Next, we'll plot these results\n",
    "    tmp = {}\n",
    "    label_idx = [tmp.setdefault(name, len(tmp)) for name in labels]\n",
    "    unique_labels = functools.reduce(lambda l, x: l if x in l else l+[x], labels, [])\n",
    "    df = pd.DataFrame({'x':xs, 'y': ys, 'label':label_idx})\n",
    "    colors = ['r', 'b', 'g', 'm', 'k']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10)) \n",
    "    for idx in range(len(set(labels))):\n",
    "      ax.scatter(df[df['label']==idx].x, df[df['label']==idx].y, c=colors[idx], label=unique_labels[idx])\n",
    "\n",
    "    ax.set(title='PCA Representation for Genres: {}'.format(unique_labels))\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgfJKubDAAsS"
   },
   "outputs": [],
   "source": [
    "def embed_movie_plots(data_text, train_or_test='train'):\n",
    "    chunk_size = 1000\n",
    "    num_chunks = math.ceil(data_text.shape[0]/chunk_size)\n",
    "    for idx in range(num_chunks):\n",
    "        with tf.Session() as session:\n",
    "            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "            data_vector  =  session.run(embed(list(data_text.iloc[idx*chunk_size:(idx+1)*chunk_size]))) #Generates the sentence embeddings\n",
    "            np.save(base_dir+'Data/preprocessed/embed_vector/movies_genres_'+train_or_test+'_vector_'+str(idx)+'.npy', data_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gMMMuhxHaG9j"
   },
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FmtyD9RgXRO"
   },
   "source": [
    "**Loading the input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T19:02:23.465051Z",
     "start_time": "2019-03-12T19:02:17.601558Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5OwlamR5aG9l"
   },
   "outputs": [],
   "source": [
    "#mydata_train = pd.read_csv('./../Data/preprocessed/movies_genres_train_preprocessed.csv')\n",
    "#mydata_test = pd.read_csv('./../Data/preprocessed/movies_genres_test_preprocessed.csv')\n",
    "#mydata = pd.read_csv('../Data/movies_genres.csv', delimiter='\\t')\n",
    "\n",
    "mydata_train = pd.read_csv(base_dir+'Data/preprocessed/movies_genres_train_preprocessed.csv')\n",
    "mydata_test = pd.read_csv(base_dir+'Data/preprocessed/movies_genres_test_preprocessed.csv')\n",
    "mydata = pd.read_csv(base_dir+'Data/movies_genres.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QgC2z4bxaG9t"
   },
   "outputs": [],
   "source": [
    "train_X, train_y = mydata_train['plot'], mydata_train.drop(['title', 'plot', 'plot_lang'], axis=1)\n",
    "test_X, test_y = mydata_test['plot'], mydata_test.drop(['title', 'plot', 'plot_lang'], axis=1)\n",
    "\n",
    "category_columns = train_y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "X5f4Bz2uaG-F",
    "outputId": "bcdccf19-a508-4b5f-b498-4c06fe325571"
   },
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
    "embed = hub.Module(module_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zFCTARoBEUT"
   },
   "source": [
    "## Obtain Plot Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qA10A8tABC1l"
   },
   "outputs": [],
   "source": [
    "# embed_movie_plots(train_X, train_or_test='train')\n",
    "# embed_movie_plots(test_X, train_or_test='test')\n",
    "\n",
    "train_files = glob.glob(base_dir+\"Data/preprocessed/embed_vector/*train*.npy\")\n",
    "train_vector_set = []\n",
    "for file in train_files:\n",
    "  train_vector_set.append(np.load(file))\n",
    "train_vector = np.concatenate(train_vector_set)\n",
    "\n",
    "test_files = glob.glob(base_dir+\"Data/preprocessed/embed_vector/*test*.npy\")\n",
    "test_vector_set = []\n",
    "for file in test_files:\n",
    "    test_vector_set.append(np.load(file))\n",
    "test_vector = np.concatenate(test_vector_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gmw9fA9sgoKE"
   },
   "source": [
    "## Visualizing Plot Similarities\n",
    "Let us pick plots from few genres and see how this universal sentence encoder embeds them into vectors.  \n",
    "**Heatmap Visualization** \n",
    "* We pick the first 50 plots from 3 genres (150 plots in all) \n",
    "* Using the embedded vectors for these we plot the heatmap\n",
    "* Plots from similar genre are expected to show higher correlation\n",
    "\n",
    "**PCA 2D Representation Visualization**\n",
    "* We pick the first 50 plots from 3 genres (150 plots in all) \n",
    "* We obtain the 2 largest variance components using PCA for these 150 vectors and project them along these components\n",
    "* Plots from similar genre are expected to be closeby and form a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1238
    },
    "colab_type": "code",
    "id": "bqORaBCfG79i",
    "outputId": "6ddd0e99-a262-487a-8ea0-2fb582353809"
   },
   "outputs": [],
   "source": [
    "genre_list = ['Adult', 'Crime', 'War']\n",
    "plot_inp_set = []\n",
    "genre_inp = []\n",
    "for genre in genre_list:\n",
    "    plot_inp_set.append(train_vector[train_y[genre]==1][:50])\n",
    "    genre_inp+=[genre]*50\n",
    "\n",
    "plot_inp=np.concatenate(plot_inp_set)\n",
    "movie_genre_heatmap(plot_inp, genre_inp) \n",
    "movie_genre_scatter_plot(plot_inp, genre_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1278
    },
    "colab_type": "code",
    "id": "mxdyaWpdKTNg",
    "outputId": "0ab12e7e-c51c-4777-c28c-16b99c2354f9"
   },
   "outputs": [],
   "source": [
    "genre_list = [ 'Action', 'Horror', 'Game-Show']\n",
    "plot_inp_set = []\n",
    "genre_inp = []\n",
    "for genre in genre_list:\n",
    "    plot_inp_set.append(train_vector[train_y[genre]==1][:50])\n",
    "    genre_inp+=[genre]*50\n",
    "\n",
    "plot_inp=np.concatenate(plot_inp_set)\n",
    "movie_genre_heatmap(plot_inp, genre_inp) \n",
    "movie_genre_scatter_plot(plot_inp, genre_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1254
    },
    "colab_type": "code",
    "id": "I2nT-8ZPLBVR",
    "outputId": "66972985-4668-4f9d-ac0a-f86f663af133"
   },
   "outputs": [],
   "source": [
    "genre_list = ['Western', 'Sport', 'Musical']\n",
    "plot_inp_set = []\n",
    "genre_inp = []\n",
    "for genre in genre_list:\n",
    "    plot_inp_set.append(train_vector[train_y[genre]==1][:50])\n",
    "    genre_inp+=[genre]*50\n",
    "\n",
    "plot_inp=np.concatenate(plot_inp_set)\n",
    "movie_genre_heatmap(plot_inp, genre_inp) \n",
    "movie_genre_scatter_plot(plot_inp, genre_inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Dasd4pGCawd"
   },
   "source": [
    "## Prediction via Cosine Similarity\n",
    "\n",
    "From the above plots, we noticed that similar genre movies have similar embedding vectors. In this method, we aim to predict the genres of the movie using a cosine similarity function as described in the below\n",
    "* Obtain the mean vector representation for each of the unique label combinations. \n",
    "  * There are 1505 unique genre combinations\n",
    "  * For each of these 1505 combinations, get a representative embedding vector = mean of all the vectors with similar genre combination\n",
    "* For each movie plot, get a cosine similarity with each of the 1505 genre combinations\n",
    "* Assign the genre combination which yields the maximum cosine similarity\n",
    "\n",
    "\n",
    "Disadvantage\n",
    "* Very high prediction time\n",
    "  * First, we will have to load the Universal Sentence Encoder\n",
    "  * Encode the plot into a 512 vector\n",
    "  * Get cosine similarity of the vector with 1505 genre combinations\n",
    "* High memory too\n",
    "  * We will have to store 1505X512 sized Look-up Table (LUT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yEpqefTEzy-X"
   },
   "outputs": [],
   "source": [
    "def cosine(s1, s2):\n",
    "  \"\"\"Take two pd.Series objects and return their cosine similarity.\"\"\"\n",
    "  return np.sum(s1 * s2) / np.sqrt(np.sum(s1 ** 2) * np.sum(s2 ** 2))\n",
    "\n",
    "def max_cosine(vector, label_embedding):\n",
    "  '''Function returns the label_embedding index with maximum cosine \n",
    "  similarity with the input vector '''\n",
    "  max_idx, max_cosine_sim = -1, float('-inf')\n",
    "  if vector['index']%1000==0:\n",
    "    print('Processing plot idx {0}'.format(vector['index']))\n",
    "  for idx in range(label_embedding.shape[0]):\n",
    "    curr_sim = cosine(vector[1:], label_embedding.iloc[idx,:])\n",
    "    (max_idx, max_cosine_sim) = (idx, curr_sim) if curr_sim>max_cosine_sim else (max_idx, max_cosine_sim)\n",
    "  return pd.Series([max_idx, max_cosine_sim])\n",
    "\n",
    "def max_cosine_unit_test(vector, label_embedding):\n",
    "  '''Used for debugging individual vectors. Returns the entire cosine\n",
    "  similarity array'''\n",
    "  cosine_list = []\n",
    "  for idx in range(label_embedding.shape[0]):\n",
    "    cosine_list.append(cosine(vector, label_embedding.iloc[idx,:]))\n",
    "  return cosine_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v47z75RBl2p3"
   },
   "source": [
    "Now let us prepare the required labels to embedding vector LUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgQd1s9F1_dk"
   },
   "outputs": [],
   "source": [
    "# Creating a LUT for the 1505 labels\n",
    "train_y_labels= train_y.groupby(list(category_columns)).ngroup()\n",
    "y_labels_lut = train_y.copy(deep=True) \n",
    "y_labels_lut['Labels'] = train_y_labels\n",
    "y_labels_lut = y_labels_lut.drop_duplicates()\n",
    "y_labels_lut = y_labels_lut.reset_index(drop=True).set_index('Labels').sort_index()\n",
    "\n",
    "# Getting the embedding vectors for each of the unique 1505 labels\n",
    "train_vector_df = pd.DataFrame(train_vector)\n",
    "train_vector_df['Labels'] = train_y_labels\n",
    "label_embedding = train_vector_df.groupby('Labels').mean()\n",
    "\n",
    "# Converting the vectors into dataframe so that we can use the apply method\n",
    "test_vector_df = pd.DataFrame(test_vector)\n",
    "test_vector_df = test_vector_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-7mj5LnmFis"
   },
   "source": [
    "Run cosine similarity make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "eRpsgELxM8rF",
    "outputId": "4e487bb8-ba3f-446d-a72c-8f3341eba192"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred_df = test_vector_df.apply(max_cosine, label_embedding=label_embedding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917
    },
    "colab_type": "code",
    "id": "J8bGv3jE9_pt",
    "outputId": "8c6ca923-360e-4f35-8cd9-39e71bcc6ad4"
   },
   "outputs": [],
   "source": [
    "predictions = pd.merge(y_pred_df, y_labels_lut, how='left', left_on=0, right_on='Labels')[category_columns]\n",
    "accuracy(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwsKp_oHmtWV"
   },
   "source": [
    "**Observations**  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "4_Modeling_Embedding_Cosine_Sim.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
